{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f36b1e05",
   "metadata": {},
   "source": [
    "## Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b80113a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0aa4fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                               text label  \\\n",
      "0   1  Chính phủ ra lệnh cấm sử dụng mạng xã hội Face...  fake   \n",
      "1   2  \\tNgân hàng Vietcombank thông báo tài khoản củ...  fake   \n",
      "2   3  TẬP ĐOÀN YONGYI CẦN TUYỂN GẤP CÁC VỊ TRÍ : SAL...  fake   \n",
      "3   4  \\tThông báo: Vui lòng cập nhật thông tin tài k...  fake   \n",
      "4   5  \\tBạn có cuộc gọi nhỡ từ số 18006666, hãy gọi ...  fake   \n",
      "\n",
      "                                    Meta_Llama3.1_8B  \n",
      "0  fake\\n    Kết quả kiểm tra bổ sung: Không có t...  \n",
      "1  fake\\n    \"\"\"\\ndef check_news(news):\\n    if n...  \n",
      "2  fake  ```python\\nimport re\\n\\ndef is_real_or_f...  \n",
      "3  fake\\n\\n    Thông tin cần xác thực: \\tThông bá...  \n",
      "4  fake\\n\\n    Thông tin cần xác thực: \\tBạn có c...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/mnt/c/Users/Phuc/Documents/Course/data/dataset_tested/test_output_Meta_Llama3.1_8B.csv')\n",
    "\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "456e5319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(text):\n",
    "    if not isinstance(text, str):\n",
    "        return 'error'\n",
    "    text_lower = text.lower()\n",
    "    has_fake = 'fake' in text_lower\n",
    "    has_real = 'real' in text_lower\n",
    "\n",
    "    if has_fake and not has_real:\n",
    "        return 'fake'\n",
    "    elif has_real and not has_fake:\n",
    "        return 'real'\n",
    "    else:\n",
    "        return 'error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b20f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Meta_Llama3.1_8B_label'] = df['Meta_Llama3.1_8B'].apply(convert_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6fed0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('/mnt/c/Users/Phuc/Documents/Course/data/dataset_tested/output_processed.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3c2057",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b076e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9a5268dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = pd.read_excel('/mnt/c/Users/Phuc/Documents/Course/data/dataset_tested/total.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "33acecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "def evaluate_label(df, true_col='label', pred_col='Meta_Llama3.1_8B_label'):\n",
    "    # Thay giá trị 'error' thành một nhãn không hợp lệ (ví dụ 'wrong') để tính sai\n",
    "    df_eval = df.copy()\n",
    "    df_eval[pred_col] = df_eval[pred_col].replace('error', 'wrong')\n",
    "    \n",
    "    # Lấy danh sách nhãn thực tế và dự đoán\n",
    "    y_true = df_eval[true_col].values\n",
    "    y_pred = df_eval[pred_col].values\n",
    "    \n",
    "    # Nếu nhãn true cũng có 'wrong' thì phải xác định label cho confusion matrix\n",
    "    labels = ['fake', 'real', 'wrong']\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, labels=['fake', 'real'], average='weighted', zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Weighted F1-score: {f1:.4f}\")\n",
    "    print(f\"Confusion Matrix (labels = {labels}):\")\n",
    "    print(cm)\n",
    "    \n",
    "    return acc, f1, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ac7ec8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6327\n",
      "Weighted F1-score: 0.6729\n",
      "Confusion Matrix (labels = ['fake', 'real', 'wrong']):\n",
      "[[336  81  37]\n",
      " [115 198  77]\n",
      " [  0   0   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6327014218009479,\n",
       " 0.6729452963174359,\n",
       " array([[336,  81,  37],\n",
       "        [115, 198,  77],\n",
       "        [  0,   0,   0]]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_label(df_total, true_col='label', pred_col='Llama3.2_3B_detected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e88d9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8009\n",
      "Weighted F1-score: 0.8142\n",
      "Confusion Matrix (labels = ['fake', 'real', 'wrong']):\n",
      "[[354  96   4]\n",
      " [ 46 322  22]\n",
      " [  0   0   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8009478672985783,\n",
       " 0.8142484112557676,\n",
       " array([[354,  96,   4],\n",
       "        [ 46, 322,  22],\n",
       "        [  0,   0,   0]]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_label(df_total, true_col='label', pred_col='Llama3.2_3B_finetuned_detected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a0136e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6623\n",
      "Weighted F1-score: 0.6344\n",
      "Confusion Matrix (labels = ['fake', 'real', 'wrong']):\n",
      "[[432  16   6]\n",
      " [236 127  27]\n",
      " [  0   0   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6623222748815166,\n",
       " 0.6344288348476915,\n",
       " array([[432,  16,   6],\n",
       "        [236, 127,  27],\n",
       "        [  0,   0,   0]]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_label(df_total, true_col='label', pred_col='Qwen2.5_3B_detected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "59f7563c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8187\n",
      "Weighted F1-score: 0.8216\n",
      "Confusion Matrix (labels = ['fake', 'real', 'wrong']):\n",
      "[[423  29   2]\n",
      " [110 268  12]\n",
      " [  0   0   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8187203791469194,\n",
       " 0.8215904420916942,\n",
       " array([[423,  29,   2],\n",
       "        [110, 268,  12],\n",
       "        [  0,   0,   0]]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_label(df_total, true_col='label', pred_col='Qwen2.5_3B_finetuned_detected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e44110bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7737\n",
      "Weighted F1-score: 0.7809\n",
      "Confusion Matrix (labels = ['fake', 'real', 'wrong']):\n",
      "[[385  68   1]\n",
      " [103 268  19]\n",
      " [  0   0   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.773696682464455,\n",
       " 0.7808506140613704,\n",
       " array([[385,  68,   1],\n",
       "        [103, 268,  19],\n",
       "        [  0,   0,   0]]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_label(df_total, true_col='label', pred_col='Gemma3_4B_detected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "51cacd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6090\n",
      "Weighted F1-score: 0.5678\n",
      "Confusion Matrix (labels = ['fake', 'real', 'wrong']):\n",
      "[[421  20  13]\n",
      " [266  93  31]\n",
      " [  0   0   0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6090047393364929,\n",
       " 0.5678242105780443,\n",
       " array([[421,  20,  13],\n",
       "        [266,  93,  31],\n",
       "        [  0,   0,   0]]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_label(df_total, true_col='label', pred_col='Meta_Llama3.1_8B_detected')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
